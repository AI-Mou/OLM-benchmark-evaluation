{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riGNbcKn-b5P"
      },
      "source": [
        "This notebook shows how to use optimum-benchmark to benchmark LLMs. It  focuses on benchmarking quantization algorithms for Mistral 7B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow56uKbJ0IGO"
      },
      "source": [
        "We need to install the following packages. bitsandbytes, auto-gptq and autoawq are only necessary if you benchmark models quantized with these algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUJH8FMq-Yx_",
        "outputId": "7a41cb94-1df3-4323-8807-e8930638d244"
      },
      "outputs": [],
      "source": [
        "!python -m pip install git+https://github.com/huggingface/optimum-benchmark.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO6NNcn90ZNd"
      },
      "source": [
        "Define the configuration for optimum-benchmark.\n",
        "\n",
        "Here we benchmark for inference, using different batch sizes, Mistral 7B loaded as fp16.\n",
        "If you run this notebook on Google Colab, you will need the A100 only for this part. The following benchmarks would run on the T4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jun 11 00:46:29 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n",
            "| N/A   34C    P0              43W / 300W |      9MiB / 81920MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a7YIfxXma5C2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 01:49:16,776\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Starting benchmark in isolated process\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:19,569\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Setting torch.distributed cuda device to 0\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:19,571\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Initializing torch.distributed process group\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:19,596\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.3.0 available.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:20,210\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - Allocating pytorch backend\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:20,210\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Benchmarking a Transformers model\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda/envs/benchmark/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:21,859\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Using automodel class AutoModelForCausalLM\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:21,859\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Seeding pytorch backend with seed 42\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:21,860\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Using AutoModel AutoModelForCausalLM\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:21,860\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating backend temporary directory\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:21,861\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with random weights\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:21,861\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating no weights model directory\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:21,861\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating no weights model state dict\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:21,862\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Saving no weights model safetensors\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:21,862\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Saving no weights model pretrained config\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:21,863\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading Transformers model using device context manager for fast initialization\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:22,180\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:22,183\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Allocating inference scenario\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:22,183\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Distributing batch size across processes\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:22,183\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:22,183\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generating Text Generation inputs\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:22,184\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing Text Generation inputs\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:22,184\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating Text Generation kwargs with default values\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:22,184\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Initializing Text Generation report\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:22,185\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for Inference\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:22,185\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up backend for Text Generation\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:26,941\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Running Text Generation memory tracking\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:26,942\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking RAM memory of process [6805]\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:26,942\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking VRAM memory of CUDA devices [0]\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:26,942\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking Allocated/Reserved memory of 1 Pytorch CUDA devices\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,074\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill memory:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,074\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max RAM: 1277.575168 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,074\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max global VRAM: 18146.459648 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,074\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max process VRAM: 17217.617920 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,074\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max reserved memory: 16693.329920 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,074\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max allocated memory: 16198.330880 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,075\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode memory:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,075\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max RAM: 1277.771776 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,075\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max global VRAM: 18146.459648 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,075\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max process VRAM: 17217.617920 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,075\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max reserved memory: 16693.329920 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,075\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max allocated memory: 16568.313344 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,075\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Running Per-Token Text Generation latency tracking\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:49:39,075\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking latency using Pytorch CUDA events\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,792\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,792\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 10\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,792\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 2.561600 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,792\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 0.256160 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,792\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.000663 s (0.26%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,792\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 0.256025 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,792\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 0.257027 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 0.257057 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 0.257081 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 10\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 24.147647 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 2.414765 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.018349 s (0.76%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 2.411397 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 2.428808 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 2.445951 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 2.459666 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,793\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ per_token latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,794\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 989\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,794\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 26.427250 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,794\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 0.026721 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,794\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.024501 s (91.69%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,794\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 0.024173 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,794\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 0.025636 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,794\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 0.026416 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,794\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 0.027357 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,795\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill throughput: 1998.750907 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,795\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode throughput: 40.997783 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,795\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ per_token throughput: 37.423493 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,799\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Benchmark completed successfully\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,800\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Destroying torch.distributed process group\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:50:05,818\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Exiting rank process\u001b[0m\n",
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 01:50:17,828\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Sending outputs to main process\u001b[0m\n",
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 01:50:17,828\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Exiting isolated process\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from optimum_benchmark import Benchmark, BenchmarkConfig, TorchrunConfig, InferenceConfig, PyTorchConfig  \n",
        "from optimum_benchmark.logging_utils import setup_logging  \n",
        "  \n",
        "# setup_logging(level=\"INFO\", handlers=[\"console\"])  \n",
        "if __name__ == \"__main__\":  \n",
        "    launcher_config = TorchrunConfig(nproc_per_node=1)  \n",
        "      \n",
        "    # 设置推理配置，包括批处理大小和序列长度等参数  \n",
        "    scenario_config = InferenceConfig(  \n",
        "        latency=True,  \n",
        "        memory=True,  \n",
        "        warmup_runs=5,  \n",
        "        input_shapes={  \n",
        "            \"batch_size\": 1,  # 设置批处理大小  \n",
        "            \"sequence_length\": 512  # 设置序列长度  \n",
        "        },  \n",
        "        generate_kwargs={  \n",
        "            \"max_new_tokens\": 100,  # 默认生成的新标记的最大数量  \n",
        "            \"min_new_tokens\": 100,  # 默认生成的新标记的最小数量  \n",
        "            \"num_beams\": 1,  # 默认束搜索数量  \n",
        "            \"temperature\": 1.0,  # 默认温度  \n",
        "            \"top_k\": 50,  # 默认Top-k采样  \n",
        "            \"top_p\": 0.9,  # 默认Top-p采样  \n",
        "            \"do_sample\": True  # 启用采样模式 \n",
        "        }  \n",
        "    ) \n",
        "      \n",
        "    # 定义 PyTorch 后端配置  \n",
        "    backend_config = PyTorchConfig(  \n",
        "        model=\"microsoft/Phi-3-mini-4k-instruct\",  \n",
        "        device=\"cuda\",  \n",
        "        device_ids=\"0\",  # 确保设备ID为字符串  \n",
        "        no_weights=True  \n",
        "    )  \n",
        "      \n",
        "    # 定义基准测试配置  \n",
        "    benchmark_config = BenchmarkConfig(  \n",
        "        name=\"pytorch_Phi-3-mini-4k-instruct\",  \n",
        "        scenario=scenario_config,  \n",
        "        launcher=launcher_config,  \n",
        "        backend=backend_config,  \n",
        "    )  \n",
        "      \n",
        "    # 运行基准测试  \n",
        "    benchmark_report = Benchmark.launch(benchmark_config)  \n",
        "      \n",
        "    # 在终端中记录基准测试结果  \n",
        "    benchmark_report.log()  # or print(benchmark_report)  \n",
        "      \n",
        "    # 将工件转换为字典或数据帧  \n",
        "    benchmark_config.to_dict()  # or benchmark_config.to_dataframe()  \n",
        "      \n",
        "    # 将工件保存到磁盘为 JSON 或 CSV 文件  \n",
        "    benchmark_report.save_csv(\"benchmark_report_pytorch_Phi-3-mini-4k-instruct.csv\")  # 保存为 CSV 文件  \n",
        "    benchmark_report.save_json(\"benchmark_report_pytorch_Phi-3-mini-4k-instruct.json\")  # 保存为 JSON 文件  \n",
        "      \n",
        "    # 或者将它们合并到一个单一的工件中  \n",
        "    benchmark = Benchmark(config=benchmark_config, report=benchmark_report)  \n",
        "    benchmark.save_json(\"benchmark_pytorch_Phi-3-mini-4k-instruct.json\")  # 保存为 JSON 文件  \n",
        "    benchmark.save_csv(\"benchmark_pytorch_Phi-3-mini-4k-instruct.csv\")  # 保存为 CSV 文件  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 01:47:27,886\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Starting benchmark in isolated process\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:30,675\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Setting torch.distributed cuda device to 0\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:30,677\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Initializing torch.distributed process group\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:30,966\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.3.0 available.\u001b[0m\n",
            "INFO 06-11 01:47:31 base.py:41] Allocating vllm backend\n",
            "INFO 06-11 01:47:31 base.py:60] \t+ Benchmarking a Transformers model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda/envs/benchmark/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 06-11 01:47:32 base.py:74] \t+ Using automodel class AutoModelForCausalLM\n",
            "INFO 06-11 01:47:32 base.py:78] \t+ Seeding vllm backend with seed 42\n",
            "INFO 06-11 01:47:32 backend.py:23] \t+ Creating backend temporary directory\n",
            "INFO 06-11 01:47:32 backend.py:27] \t+ Loading no weights model\n",
            "INFO 06-11 01:47:32 backend.py:90] \t+ Creating no weights model\n",
            "INFO 06-11 01:47:32 backend.py:59] \t+ Creating no weights model directory\n",
            "INFO 06-11 01:47:32 backend.py:61] \t+ Creating no weights model state dict\n",
            "INFO 06-11 01:47:32 backend.py:63] \t+ Saving no weights model safetensors\n",
            "INFO 06-11 01:47:32 backend.py:66] \t+ Saving no weights model pretrained config\n",
            "INFO 06-11 01:47:32 backend.py:68] \t+ Saving no weights model pretrained processor\n",
            "INFO 06-11 01:47:32 backend.py:71] \t+ Loading no weights model from /tmp/tmpsag0weo1/no_weights_model\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:33,235\u001b[0m][\u001b[34maccelerate.utils.modeling\u001b[0m][\u001b[32mINFO\u001b[0m] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\u001b[0m\n",
            "INFO 06-11 01:47:34 backend.py:76] \t+ Saving no weights model\n",
            "INFO 06-11 01:47:43 backend.py:82] \t+ Modifying generation config for fixed length generation\n",
            "INFO 06-11 01:47:43 backend.py:86] \t+ Saving new pretrained generation config\n",
            "INFO 06-11 01:47:43 backend.py:94] \t+ Loading no weights model\n",
            "INFO 06-11 01:47:43 config.py:1130] Casting torch.float32 to torch.float16.\n",
            "INFO 06-11 01:47:43 config.py:1151] Downcasting torch.float32 to torch.float16.\n",
            "INFO 06-11 01:47:43 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/tmp/tmpsag0weo1/no_weights_model', speculative_config=None, tokenizer='microsoft/Phi-3-mini-4k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=42, served_model_name=/tmp/tmpsag0weo1/no_weights_model)\n",
            "INFO 06-11 01:47:43 selector.py:139] Cannot use FlashAttention-2 backend due to sliding window.\n",
            "INFO 06-11 01:47:43 selector.py:51] Using XFormers backend.\n",
            "INFO 06-11 01:47:44 selector.py:139] Cannot use FlashAttention-2 backend due to sliding window.\n",
            "INFO 06-11 01:47:44 selector.py:51] Using XFormers backend.\n",
            "INFO 06-11 01:47:45 model_runner.py:146] Loading model weights took 7.1183 GB\n",
            "INFO 06-11 01:47:46 gpu_executor.py:83] # GPU blocks: 10860, # CPU blocks: 682\n",
            "INFO 06-11 01:47:47 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 06-11 01:47:47 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 06-11 01:47:51 model_runner.py:924] Graph capturing finished in 4 secs.\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,314\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Allocating inference scenario\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,315\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Distributing batch size across processes\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,315\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,315\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generating Text Generation inputs\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,315\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing Text Generation inputs\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,316\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating Text Generation kwargs with default values\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,316\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Initializing Text Generation report\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,316\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for Inference\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,317\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up backend for Text Generation\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,952\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Running Text Generation memory tracking\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,952\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking RAM memory of process [6454]\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:47:52,952\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking VRAM memory of CUDA devices [0]\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:02,683\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill memory:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:02,683\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max RAM: 5928.034304 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:02,683\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max global VRAM: 78605.254656 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:02,683\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max process VRAM: 77668.024320 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:02,683\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode memory:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:02,683\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max RAM: 5928.034304 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:02,683\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max global VRAM: 78605.254656 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:02,683\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max process VRAM: 77668.024320 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:02,684\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Running Text Generation latency tracking\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:02,684\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking latency using CPU performance counter\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,077\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,077\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 369\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,077\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 9.928099 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,077\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 0.026905 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,077\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.000406 s (1.51%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,077\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 0.026875 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,077\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 0.027460 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,077\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 0.027574 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,078\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 0.027824 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,078\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,078\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 23\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,078\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 9.761895 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,078\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 0.424430 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,078\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.001034 s (0.24%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,078\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 0.424267 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,078\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 0.425432 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,078\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 0.425644 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,078\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 0.427334 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,079\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill throughput: 19029.623959 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,079\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode throughput: 115.448896 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,083\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Benchmark completed successfully\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,083\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Destroying torch.distributed process group\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 01:48:23,104\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Exiting rank process\u001b[0m\n",
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 01:48:29,540\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Sending outputs to main process\u001b[0m\n",
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 01:48:29,541\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Exiting isolated process\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from optimum_benchmark import Benchmark, BenchmarkConfig, TorchrunConfig, InferenceConfig, VLLMConfig  \n",
        "from optimum_benchmark.logging_utils import setup_logging  \n",
        "  \n",
        "# setup_logging(level=\"INFO\", handlers=[\"console\"])  \n",
        "if __name__ == \"__main__\":  \n",
        "    launcher_config = TorchrunConfig(nproc_per_node=1)  \n",
        "      \n",
        "    # 设置推理配置，包括batch size、sequence length等参数  \n",
        "    scenario_config = InferenceConfig(  \n",
        "        latency=True,  \n",
        "        memory=True,  \n",
        "        warmup_runs=5,  \n",
        "        input_shapes={  \n",
        "            \"batch_size\": 1,  # 默认批处理大小  \n",
        "            \"sequence_length\": 512  # 默认序列长度  \n",
        "        },  \n",
        "        generate_kwargs={  \n",
        "            \"max_new_tokens\": 50,  # 默认生成的新标记的最大数量  \n",
        "            \"min_new_tokens\": 50,  # 默认生成的新标记的最小数量  \n",
        "            \"num_beams\": 1,  # 默认束搜索数量  \n",
        "            \"temperature\": 1.0,  # 默认温度  \n",
        "            \"top_k\": 50,  # 默认Top-k采样  \n",
        "            \"top_p\": 0.9,  # 默认Top-p采样  \n",
        "            \"do_sample\": True  # 启用采样模式 \n",
        "        }  \n",
        "    )  \n",
        "      \n",
        "    # 定义 vllm_config  \n",
        "    vllm_config = VLLMConfig(  \n",
        "        model=\"microsoft/Phi-3-mini-4k-instruct\",  \n",
        "        device=\"cuda\",  \n",
        "        device_ids=\"0\",  # 确保设备ID为字符串  \n",
        "        no_weights=True  \n",
        "    )  \n",
        "      \n",
        "    benchmark_config = BenchmarkConfig(  \n",
        "        name=\"vllm_phi3_mini_4k_instruct\",  \n",
        "        scenario=scenario_config,  \n",
        "        launcher=launcher_config,  \n",
        "        backend=vllm_config,  \n",
        "    )  \n",
        "      \n",
        "    benchmark_report = Benchmark.launch(benchmark_config)  \n",
        "      \n",
        "    # log the benchmark in terminal  \n",
        "    benchmark_report.log()  # or print(benchmark_report)  \n",
        "      \n",
        "    # convert artifacts to a dictionary or dataframe  \n",
        "    benchmark_config.to_dict()  # or benchmark_config.to_dataframe()  \n",
        "      \n",
        "    # save artifacts to disk as json or csv files  \n",
        "    benchmark_report.save_csv(\"benchmark_reportvllm.Phi-3-mini-4k-instruct.csv\")  # 保存为 CSV 文件  \n",
        "    benchmark_report.save_json(\"benchmark_reportvllm.Phi-3-mini-4k-instruct.json\")  # 保存为 JSON 文件  \n",
        "      \n",
        "    # or merge them into a single artifact  \n",
        "    benchmark = Benchmark(config=benchmark_config, report=benchmark_report)  \n",
        "    benchmark.save_json(\"benchmarkvllm.Phi-3-mini-4k-instruct.json\")  # 保存为 JSON 文件  \n",
        "    benchmark.save_csv(\"benchmarkvllm.Phi-3-mini-4k-instruct.csv\")  # 保存为 CSV 文件  \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
